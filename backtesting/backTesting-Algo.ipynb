{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Aviral\\GITHUB\\Price-Prediction\\myenv\\Lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:473: ValueWarning: A date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\n",
      "  self._init_dates(dates, freq)\n",
      "c:\\Aviral\\GITHUB\\Price-Prediction\\myenv\\Lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:473: ValueWarning: A date index has been provided, but it is not monotonic and so will be ignored when e.g. forecasting.\n",
      "  self._init_dates(dates, freq)\n",
      "c:\\Aviral\\GITHUB\\Price-Prediction\\myenv\\Lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:473: ValueWarning: A date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\n",
      "  self._init_dates(dates, freq)\n",
      "c:\\Aviral\\GITHUB\\Price-Prediction\\myenv\\Lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:473: ValueWarning: A date index has been provided, but it is not monotonic and so will be ignored when e.g. forecasting.\n",
      "  self._init_dates(dates, freq)\n",
      "c:\\Aviral\\GITHUB\\Price-Prediction\\myenv\\Lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:473: ValueWarning: A date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\n",
      "  self._init_dates(dates, freq)\n",
      "c:\\Aviral\\GITHUB\\Price-Prediction\\myenv\\Lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:473: ValueWarning: A date index has been provided, but it is not monotonic and so will be ignored when e.g. forecasting.\n",
      "  self._init_dates(dates, freq)\n",
      "c:\\Aviral\\GITHUB\\Price-Prediction\\myenv\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m23260/23260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m248s\u001b[0m 11ms/step - loss: 29133644.0000 - val_loss: 27892294.0000\n",
      "Epoch 2/20\n",
      "\u001b[1m23260/23260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m248s\u001b[0m 11ms/step - loss: 25720322.0000 - val_loss: 26714006.0000\n",
      "Epoch 3/20\n",
      "\u001b[1m23260/23260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m233s\u001b[0m 10ms/step - loss: 25640202.0000 - val_loss: 25175744.0000\n",
      "Epoch 4/20\n",
      "\u001b[1m23260/23260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m262s\u001b[0m 11ms/step - loss: 25535236.0000 - val_loss: 25861092.0000\n",
      "Epoch 5/20\n",
      "\u001b[1m23260/23260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m237s\u001b[0m 10ms/step - loss: 25498434.0000 - val_loss: 25187614.0000\n",
      "Epoch 6/20\n",
      "\u001b[1m23260/23260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m218s\u001b[0m 9ms/step - loss: 25490868.0000 - val_loss: 25176562.0000\n",
      "Epoch 7/20\n",
      "\u001b[1m23260/23260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m226s\u001b[0m 10ms/step - loss: 25429938.0000 - val_loss: 25695130.0000\n",
      "Epoch 8/20\n",
      "\u001b[1m23260/23260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m218s\u001b[0m 9ms/step - loss: 25415380.0000 - val_loss: 25443912.0000\n",
      "Epoch 9/20\n",
      "\u001b[1m23260/23260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m225s\u001b[0m 10ms/step - loss: 25400236.0000 - val_loss: 25546978.0000\n",
      "Epoch 10/20\n",
      "\u001b[1m23260/23260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m231s\u001b[0m 10ms/step - loss: 25352198.0000 - val_loss: 25285652.0000\n",
      "Epoch 11/20\n",
      "\u001b[1m23260/23260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m245s\u001b[0m 11ms/step - loss: 25397078.0000 - val_loss: 25406932.0000\n",
      "Epoch 12/20\n",
      "\u001b[1m23260/23260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m219s\u001b[0m 9ms/step - loss: 25410694.0000 - val_loss: 25233346.0000\n",
      "Epoch 13/20\n",
      "\u001b[1m23260/23260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m235s\u001b[0m 10ms/step - loss: 25485560.0000 - val_loss: 25959114.0000\n",
      "Epoch 14/20\n",
      "\u001b[1m23260/23260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m245s\u001b[0m 11ms/step - loss: 25319414.0000 - val_loss: 25173494.0000\n",
      "Epoch 15/20\n",
      "\u001b[1m23260/23260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m233s\u001b[0m 10ms/step - loss: 25308412.0000 - val_loss: 25178618.0000\n",
      "Epoch 16/20\n",
      "\u001b[1m23260/23260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m241s\u001b[0m 10ms/step - loss: 25369210.0000 - val_loss: 25184526.0000\n",
      "Epoch 17/20\n",
      "\u001b[1m23260/23260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m237s\u001b[0m 10ms/step - loss: 25358090.0000 - val_loss: 25192458.0000\n",
      "Epoch 18/20\n",
      "\u001b[1m23260/23260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m212s\u001b[0m 9ms/step - loss: 25346294.0000 - val_loss: 25272756.0000\n",
      "Epoch 19/20\n",
      "\u001b[1m23260/23260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m208s\u001b[0m 9ms/step - loss: 25327886.0000 - val_loss: 25381960.0000\n",
      "Epoch 20/20\n",
      "\u001b[1m23260/23260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m209s\u001b[0m 9ms/step - loss: 25300886.0000 - val_loss: 25378166.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Aviral\\GITHUB\\Price-Prediction\\myenv\\Lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:837: ValueWarning: No supported index is available. Prediction results will be given with an integer index beginning at `start`.\n",
      "  return get_prediction_index(\n",
      "c:\\Aviral\\GITHUB\\Price-Prediction\\myenv\\Lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:837: FutureWarning: No supported index is available. In the next version, calling this method in a model without a supported index will result in an exception.\n",
      "  return get_prediction_index(\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "-1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Aviral\\GITHUB\\Price-Prediction\\myenv\\Lib\\site-packages\\pandas\\core\\indexes\\range.py:413\u001b[0m, in \u001b[0;36mRangeIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    412\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 413\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_range\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    414\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[1;31mValueError\u001b[0m: -1 is not in range",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 76\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     75\u001b[0m     file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/nifty2015-2025.csv\u001b[39m\u001b[38;5;124m'\u001b[39m  \u001b[38;5;66;03m# file path\u001b[39;00m\n\u001b[1;32m---> 76\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[11], line 70\u001b[0m, in \u001b[0;36mmain\u001b[1;34m(file_path)\u001b[0m\n\u001b[0;32m     67\u001b[0m lstm_model\u001b[38;5;241m.\u001b[39mfit(X_train, y_train, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, validation_data\u001b[38;5;241m=\u001b[39m(X_test, y_test))\n\u001b[0;32m     69\u001b[0m \u001b[38;5;66;03m# Step 8: Predict using hybrid model\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m hybrid_prediction \u001b[38;5;241m=\u001b[39m \u001b[43mpredict_hybrid\u001b[49m\u001b[43m(\u001b[49m\u001b[43marima_fit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlstm_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresiduals\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscaler\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHybrid Prediction for 15 minutes later: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhybrid_prediction\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[11], line 34\u001b[0m, in \u001b[0;36mpredict_hybrid\u001b[1;34m(arima_fit, lstm_model, data, scaler, future_steps)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpredict_hybrid\u001b[39m(arima_fit, lstm_model, data, scaler, future_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m15\u001b[39m):\n\u001b[1;32m---> 34\u001b[0m     arima_forecast \u001b[38;5;241m=\u001b[39m \u001b[43marima_fit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforecast\u001b[49m\u001b[43m(\u001b[49m\u001b[43msteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfuture_steps\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m  \u001b[38;5;66;03m# ARIMA prediction\u001b[39;00m\n\u001b[0;32m     35\u001b[0m     nn_input \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;241m-\u001b[39mfuture_steps:]  \u001b[38;5;66;03m# Last residuals for NN input\u001b[39;00m\n\u001b[0;32m     36\u001b[0m     nn_input \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39mtransform(nn_input\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Aviral\\GITHUB\\Price-Prediction\\myenv\\Lib\\site-packages\\pandas\\core\\series.py:1121\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[key]\n\u001b[0;32m   1120\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m key_is_scalar:\n\u001b[1;32m-> 1121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1123\u001b[0m \u001b[38;5;66;03m# Convert generator to list before going through hashable part\u001b[39;00m\n\u001b[0;32m   1124\u001b[0m \u001b[38;5;66;03m# (We will iterate through the generator there to check for slices)\u001b[39;00m\n\u001b[0;32m   1125\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n",
      "File \u001b[1;32mc:\\Aviral\\GITHUB\\Price-Prediction\\myenv\\Lib\\site-packages\\pandas\\core\\series.py:1237\u001b[0m, in \u001b[0;36mSeries._get_value\u001b[1;34m(self, label, takeable)\u001b[0m\n\u001b[0;32m   1234\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[label]\n\u001b[0;32m   1236\u001b[0m \u001b[38;5;66;03m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n\u001b[1;32m-> 1237\u001b[0m loc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1239\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(loc):\n\u001b[0;32m   1240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[loc]\n",
      "File \u001b[1;32mc:\\Aviral\\GITHUB\\Price-Prediction\\myenv\\Lib\\site-packages\\pandas\\core\\indexes\\range.py:415\u001b[0m, in \u001b[0;36mRangeIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    413\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_range\u001b[38;5;241m.\u001b[39mindex(new_key)\n\u001b[0;32m    414\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m--> 415\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m    416\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Hashable):\n\u001b[0;32m    417\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: -1"
     ]
    }
   ],
   "source": [
    "# Load and preprocess data\n",
    "def load_data(file_path):\n",
    "    data = pd.read_csv(file_path)\n",
    "    data['timestamp'] = pd.to_datetime(data['timestamp'])\n",
    "    data.set_index('timestamp', inplace=True)\n",
    "    return data\n",
    "\n",
    "# ARIMA model for linear patterns\n",
    "def fit_arima(data, order=(1, 1, 1)):\n",
    "    model = ARIMA(data, order=order)\n",
    "    arima_fit = model.fit()\n",
    "    return arima_fit\n",
    "\n",
    "# Prepare data for neural network\n",
    "def prepare_nn_data(residuals, original_data, future_steps=15):\n",
    "    residuals = residuals[~np.isnan(residuals)]  # Drop NaN residuals\n",
    "    X, y = [], []\n",
    "    for i in range(len(residuals) - future_steps):\n",
    "        X.append(residuals[i:i + future_steps])\n",
    "        y.append(original_data[i + future_steps])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Build LSTM neural network\n",
    "def build_lstm_model(input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(64, activation='relu', input_shape=input_shape, return_sequences=False))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(1))  # Predict single value (price)\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    return model\n",
    "\n",
    "# Combine ARIMA and LSTM predictions\n",
    "def predict_hybrid(arima_fit, lstm_model, data, scaler, future_steps=15):\n",
    "    arima_forecast = arima_fit.forecast(steps=future_steps)[-1]  # ARIMA prediction\n",
    "    nn_input = data[-future_steps:]  # Last residuals for NN input\n",
    "    nn_input = scaler.transform(nn_input.reshape(-1, 1)).reshape(1, -1, 1)\n",
    "    nn_forecast = lstm_model.predict(nn_input)\n",
    "    return arima_forecast + scaler.inverse_transform(nn_forecast).flatten()[0]\n",
    "\n",
    "# Main workflow\n",
    "def main(file_path):\n",
    "    # Step 1: Load data\n",
    "    data = load_data(file_path)\n",
    "    close_prices = data['close']\n",
    "\n",
    "    # Step 2: Fit ARIMA\n",
    "    arima_fit = fit_arima(close_prices)\n",
    "\n",
    "    # Step 3: Extract ARIMA residuals\n",
    "    residuals = arima_fit.resid\n",
    "\n",
    "    # Step 4: Scale residuals for NN\n",
    "    scaler = MinMaxScaler()\n",
    "    residuals_scaled = scaler.fit_transform(residuals.values.reshape(-1, 1))\n",
    "\n",
    "    # Step 5: Prepare data for NN\n",
    "    X, y = prepare_nn_data(residuals_scaled, close_prices.values)\n",
    "\n",
    "    # Reshape X for LSTM input (samples, timesteps, features)\n",
    "    X = X.reshape(X.shape[0], X.shape[1], 1)\n",
    "\n",
    "    # Step 6: Train-test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Step 7: Build and train LSTM model\n",
    "    lstm_model = build_lstm_model(X_train.shape[1:])\n",
    "    lstm_model.fit(X_train, y_train, epochs=20, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "    # Step 8: Predict using hybrid model\n",
    "    hybrid_prediction = predict_hybrid(arima_fit, lstm_model, residuals.values, scaler)\n",
    "    print(f\"Hybrid Prediction for 15 minutes later: {hybrid_prediction}\")\n",
    "\n",
    "# Run the script\n",
    "if __name__ == \"__main__\":\n",
    "    file_path = 'data/nifty2015-2025.csv'  # file path\n",
    "    main(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import talib\n",
    "# from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "# from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "# from statsmodels.tsa.arima.model import ARIMA\n",
    "# from arch import arch_model\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import LSTM, Dense\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# # Load and preprocess data\n",
    "# def load_and_preprocess_data(file_path, interval='15T'):\n",
    "#     df = pd.read_csv(file_path, parse_dates=['timestamp'])\n",
    "#     df['timestamp'] = df['timestamp'].dt.tz_localize(None)\n",
    "#     df.set_index('timestamp', inplace=True)\n",
    "\n",
    "#     df = df.resample(interval).agg({\n",
    "#         'open': 'first',\n",
    "#         'high': 'max',\n",
    "#         'low': 'min',\n",
    "#         'close': 'last',\n",
    "#         'volume': 'sum'\n",
    "#     }).dropna()\n",
    "\n",
    "#     scaler = StandardScaler()\n",
    "#     df[['open', 'high', 'low', 'close', 'volume']] = scaler.fit_transform(df[['open', 'high', 'low', 'close', 'volume']])\n",
    "\n",
    "#     return df, scaler\n",
    "\n",
    "# # Add technical indicators\n",
    "# def add_technical_indicators(df):\n",
    "#     df['RSI_14'] = talib.RSI(df['close'], timeperiod=14)\n",
    "#     df['MA_50'] = talib.SMA(df['close'], timeperiod=50)\n",
    "#     df['EMA_20'] = talib.EMA(df['close'], timeperiod=20)\n",
    "#     df['MACD'], df['MACD_signal'], _ = talib.MACD(df['close'], fastperiod=12, slowperiod=26, signalperiod=9)\n",
    "#     df['BB_upper'], df['BB_middle'], df['BB_lower'] = talib.BBANDS(df['close'], timeperiod=20)\n",
    "#     df.dropna(inplace=True)\n",
    "#     return df\n",
    "\n",
    "# # Create target variable\n",
    "# def create_target_variable(df, future_steps=15, threshold=3):\n",
    "#     df['future_close'] = df['close'].shift(-future_steps)\n",
    "#     df['direction'] = np.where(df['future_close'] > df['close'], 1, 0)\n",
    "#     df['price_diff'] = df['future_close'] - df['close']\n",
    "#     df['correct_movement'] = np.where(df['price_diff'].abs() <= threshold, 1, 0)\n",
    "#     df.dropna(inplace=True)\n",
    "#     return df\n",
    "\n",
    "# # Prepare data for neural network\n",
    "# def prepare_nn_data(df, feature_columns, future_steps=15):\n",
    "#     X = df[feature_columns].values\n",
    "#     y = df[['direction', 'correct_movement']].values\n",
    "#     scaler = MinMaxScaler()\n",
    "#     X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "#     X_seq, y_seq = [], []\n",
    "#     for i in range(len(X_scaled) - future_steps):\n",
    "#         X_seq.append(X_scaled[i:i + future_steps])\n",
    "#         y_seq.append(y[i + future_steps])\n",
    "\n",
    "#     return np.array(X_seq), np.array(y_seq), scaler\n",
    "\n",
    "# # Fit ARIMA and GARCH models\n",
    "# def fit_arima(data):\n",
    "#     model = ARIMA(data, order=(1,1,1))\n",
    "#     return model.fit()\n",
    "\n",
    "# def fit_garch(data):\n",
    "#     garch = arch_model(data, vol=\"Garch\", p=1, q=1)\n",
    "#     return garch.fit(disp='off')\n",
    "\n",
    "# # Build LSTM model\n",
    "# def build_lstm_model(input_shape):\n",
    "#     model = Sequential([\n",
    "#         LSTM(64, activation='relu', input_shape=input_shape, return_sequences=True),\n",
    "#         LSTM(32, activation='relu'),\n",
    "#         Dense(16, activation='relu'),\n",
    "#         Dense(2, activation='sigmoid')\n",
    "#     ])\n",
    "#     model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "#     return model\n",
    "\n",
    "# # Evaluate model performance\n",
    "# def evaluate_model(model, X_test, y_test):\n",
    "#     y_pred = (model.predict(X_test) > 0.5).astype(int)\n",
    "#     direction_accuracy = accuracy_score(y_test[:, 0], y_pred[:, 0])\n",
    "#     movement_accuracy = accuracy_score(y_test[:, 1], y_pred[:, 1])\n",
    "    \n",
    "#     print(f\"Direction Accuracy: {direction_accuracy:.2f}\")\n",
    "#     print(\"Direction Confusion Matrix:\\n\", confusion_matrix(y_test[:, 0], y_pred[:, 0]))\n",
    "#     print(f\"Price Movement Accuracy: {movement_accuracy:.2f}\")\n",
    "#     print(\"Movement Confusion Matrix:\\n\", confusion_matrix(y_test[:, 1], y_pred[:, 1]))\n",
    "\n",
    "#     return direction_accuracy, movement_accuracy\n",
    "\n",
    "# # Main workflow\n",
    "# def main(file_path):\n",
    "#     df, scaler = load_and_preprocess_data(file_path)\n",
    "#     df = add_technical_indicators(df)\n",
    "#     df = create_target_variable(df)\n",
    "\n",
    "#     feature_cols = ['open', 'high', 'low', 'close', 'volume', 'RSI_14', 'MA_50', 'EMA_20', 'MACD', 'BB_upper']\n",
    "#     X, y, scaler = prepare_nn_data(df, feature_cols)\n",
    "\n",
    "#     X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "#     # Train ARIMA and GARCH models\n",
    "#     arima_fit = fit_arima(df['close'])\n",
    "#     garch_fit = fit_garch(df['close'])\n",
    "\n",
    "#     df['GARCH_Residuals'] = garch_fit.resid\n",
    "#     df.dropna(inplace=True)\n",
    "\n",
    "#     # Train LSTM model\n",
    "#     lstm_model = build_lstm_model(X_train.shape[1:])\n",
    "#     lstm_model.fit(X_train, y_train, epochs=20, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "#     # Evaluate LSTM model\n",
    "#     dir_acc, move_acc = evaluate_model(lstm_model, X_test, y_test)\n",
    "\n",
    "#     print(f\"Final Accuracy - Direction: {dir_acc:.2f}, Price Movement: {move_acc:.2f}\")\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     file_path = 'data/nifty2015-2025.csv'\n",
    "#     main(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Aviral\\GITHUB\\Price-Prediction\\myenv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data for interval: 5T\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import talib\n",
    "import optuna\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from arch import arch_model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load and preprocess data\n",
    "def load_and_preprocess_data(file_path, interval='15T'):\n",
    "    df = pd.read_csv(file_path, parse_dates=['timestamp'])\n",
    "    df['timestamp'] = df['timestamp'].dt.tz_localize(None)\n",
    "    df.set_index('timestamp', inplace=True)\n",
    "\n",
    "    df = df.resample(interval).agg({\n",
    "        'open': 'first',\n",
    "        'high': 'max',\n",
    "        'low': 'min',\n",
    "        'close': 'last',\n",
    "        'volume': 'sum'\n",
    "    }).dropna()\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    df[['open', 'high', 'low', 'close', 'volume']] = scaler.fit_transform(df[['open', 'high', 'low', 'close', 'volume']])\n",
    "\n",
    "    return df, scaler\n",
    "\n",
    "# Add technical indicators\n",
    "def add_technical_indicators(df):\n",
    "    df['RSI_14'] = talib.RSI(df['close'], timeperiod=14)\n",
    "    df['MA_50'] = talib.SMA(df['close'], timeperiod=50)\n",
    "    df['EMA_20'] = talib.EMA(df['close'], timeperiod=20)\n",
    "    df['MACD'], df['MACD_signal'], _ = talib.MACD(df['close'], fastperiod=12, slowperiod=26, signalperiod=9)\n",
    "    df['BB_upper'], df['BB_middle'], df['BB_lower'] = talib.BBANDS(df['close'], timeperiod=20)\n",
    "    df.dropna(inplace=True)\n",
    "    return df\n",
    "\n",
    "# Create target variable\n",
    "def create_target_variable(df, future_steps=15, threshold=3):\n",
    "    df['future_close'] = df['close'].shift(-future_steps)\n",
    "    df['direction'] = np.where(df['future_close'] > df['close'], 1, 0)\n",
    "    df['price_diff'] = df['future_close'] - df['close']\n",
    "    df['correct_movement'] = np.where(df['price_diff'].abs() <= threshold, 1, 0)\n",
    "    df.dropna(inplace=True)\n",
    "    return df\n",
    "\n",
    "# Prepare data for neural network\n",
    "def prepare_nn_data(df, feature_columns, future_steps=15):\n",
    "    X = df[feature_columns].values\n",
    "    y = df[['direction', 'correct_movement']].values\n",
    "    scaler = MinMaxScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    X_seq, y_seq = [], []\n",
    "    for i in range(len(X_scaled) - future_steps):\n",
    "        X_seq.append(X_scaled[i:i + future_steps])\n",
    "        y_seq.append(y[i + future_steps])\n",
    "\n",
    "    return np.array(X_seq), np.array(y_seq), scaler\n",
    "\n",
    "# Fit ARIMA and GARCH models\n",
    "def fit_arima(data):\n",
    "    model = ARIMA(data, order=(1,1,1))\n",
    "    return model.fit()\n",
    "\n",
    "def fit_garch(data):\n",
    "    garch = arch_model(data, vol=\"Garch\", p=1, q=1)\n",
    "    return garch.fit(disp='off')\n",
    "\n",
    "# Build and train LSTM model with hyperparameter tuning\n",
    "def objective(trial):\n",
    "    n_lstm_layers = trial.suggest_int('n_lstm_layers', 1, 3)\n",
    "    lstm_units = trial.suggest_int('lstm_units', 32, 128)\n",
    "    dropout_rate = trial.suggest_float('dropout_rate', 0.1, 0.5)\n",
    "    batch_size = trial.suggest_categorical('batch_size', [16, 32, 64])\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-4, 1e-2, log=True)\n",
    "\n",
    "    model = Sequential()\n",
    "    for _ in range(n_lstm_layers):\n",
    "        model.add(LSTM(lstm_units, activation='relu', return_sequences=True))\n",
    "        model.add(Dropout(dropout_rate))\n",
    "    model.add(LSTM(lstm_units // 2, activation='relu', return_sequences=False))\n",
    "    model.add(Dense(2, activation='sigmoid'))\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_global, y_global, test_size=0.2, random_state=42)\n",
    "    model.fit(X_train, y_train, epochs=20, batch_size=batch_size, validation_data=(X_test, y_test), verbose=0)\n",
    "\n",
    "    y_pred = (model.predict(X_test) > 0.5).astype(int)\n",
    "    accuracy = accuracy_score(y_test[:, 0], y_pred[:, 0])\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "# Evaluate model performance\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    y_pred = (model.predict(X_test) > 0.5).astype(int)\n",
    "    direction_accuracy = accuracy_score(y_test[:, 0], y_pred[:, 0])\n",
    "    movement_accuracy = accuracy_score(y_test[:, 1], y_pred[:, 1])\n",
    "\n",
    "    print(f\"Direction Accuracy: {direction_accuracy:.2f}\")\n",
    "    print(\"Direction Confusion Matrix:\\n\", confusion_matrix(y_test[:, 0], y_pred[:, 0]))\n",
    "    print(f\"Price Movement Accuracy: {movement_accuracy:.2f}\")\n",
    "    print(\"Movement Confusion Matrix:\\n\", confusion_matrix(y_test[:, 1], y_pred[:, 1]))\n",
    "\n",
    "    return direction_accuracy, movement_accuracy\n",
    "\n",
    "# Main workflow\n",
    "def main(file_path):\n",
    "    global X_global, y_global\n",
    "    timeframes = ['5T', '15T', '30T']\n",
    "    best_results = {}\n",
    "\n",
    "    for interval in timeframes:\n",
    "        print(f\"Processing data for interval: {interval}\")\n",
    "        df, scaler = load_and_preprocess_data(file_path, interval)\n",
    "        df = add_technical_indicators(df)\n",
    "        df = create_target_variable(df)\n",
    "\n",
    "        feature_cols = ['open', 'high', 'low', 'close', 'volume', 'RSI_14', 'MA_50', 'EMA_20', 'MACD', 'BB_upper']\n",
    "        X, y, scaler = prepare_nn_data(df, feature_cols)\n",
    "        \n",
    "        X_global, y_global = X, y  # Save globally for Optuna\n",
    "        \n",
    "        # Optimize hyperparameters using Optuna\n",
    "        study = optuna.create_study(direction='maximize')\n",
    "        study.optimize(objective, n_trials=20)\n",
    "\n",
    "        print(f\"Best trial for {interval}: {study.best_trial.params}\")\n",
    "        best_results[interval] = study.best_trial.params\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "        # Train best LSTM model\n",
    "        best_params = study.best_trial.params\n",
    "        model = Sequential()\n",
    "        for _ in range(best_params['n_lstm_layers']):\n",
    "            model.add(LSTM(best_params['lstm_units'], activation='relu', return_sequences=True))\n",
    "            model.add(Dropout(best_params['dropout_rate']))\n",
    "        model.add(LSTM(best_params['lstm_units'] // 2, activation='relu'))\n",
    "        model.add(Dense(2, activation='sigmoid'))\n",
    "        model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        model.fit(X_train, y_train, epochs=20, batch_size=best_params['batch_size'], validation_data=(X_test, y_test))\n",
    "\n",
    "        # Evaluate model\n",
    "        dir_acc, move_acc = evaluate_model(model, X_test, y_test)\n",
    "\n",
    "    print(\"Best results across timeframes:\", best_results)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    file_path = 'data/nifty2015-2025.csv'\n",
    "    main(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
